{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRhnLjGg5zVH"
      },
      "source": [
        "# Anomaly Detection using CNN Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpdVh65W6ibR"
      },
      "source": [
        "### Loading Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUt_5pgQ_fXr"
      },
      "outputs": [],
      "source": [
        "# Unzip dataset\n",
        "!unzip -o Dataset/fruits_anomaly_detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaHyHQpq42B"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMCQsCrn6eeN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Input, Conv2D, MaxPooling2D, UpSampling2D, Rescaling\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import array_to_img, img_to_array, load_img, image_dataset_from_directory\n",
        "from PIL import Image, ImageChops\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC-Hh-ZfEfrC"
      },
      "source": [
        "### Create generators for training, validation and testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: prepare data using a generator\n",
        "seed = 91166\n",
        "batch_size = 85\n",
        "\n",
        "# TODO: define the Normalization Layer\n",
        "normalization_layer = ...\n",
        "\n",
        "# Dataset definition\n",
        "# TODO: use 'image_dataset_from_directory' to create a Dataset\n",
        "#       Split training set into training/validation\n",
        "train_dataset, validation_dataset = ...\n",
        "test_dataset = ...\n",
        "# TODO: choose eggplant as anomaly test set\n",
        "anomaly_dataset = ...\n",
        "\n",
        "# Normalization\n",
        "# TODO: define the normalization function\n",
        "# NOTE: use a lambda function λ\n",
        "_norm_function = ...\n",
        "\n",
        "# TODO: apply the normalization function to Datasets\n",
        "# NOTE: check the 'map' method\n",
        "train_dataset_norm = ...\n",
        "validation_dataset_norm = ...\n",
        "test_dataset_norm = ...\n",
        "anomaly_dataset_norm = ...\n",
        "\n",
        "# X as label (X = Y)\n",
        "# TODO: force the Dataset to return the original image (X) as the\n",
        "#       label (Y)\n",
        "# NOTE: use a lambda function λ\n",
        "_replaceY_function = ...\n",
        "train_dataset_norm = ...\n",
        "validation_dataset_norm = ...\n",
        "test_dataset_norm = ...\n",
        "anomaly_dataset_norm = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset in [train_dataset_norm, validation_dataset_norm, test_dataset_norm, anomaly_dataset_norm]:\n",
        "    print(dataset)\n",
        "    # TODO: use 'take' method to retrieve a single batch from Dataset\n",
        "    #       and plot the images (X, Y) and some stats (like shapes)\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap73VRaKq98w"
      },
      "source": [
        "## **Autoencoder Architecture**\n",
        "\n",
        "As we have seen in the case of MLP Autoencoder, we build a structure composed by an **Encoder**, that able to reduce the dimensions of our data (extract latent fetaures), and a **Decoder**, that is able to restore the original dimensions.\n",
        "\n",
        "The output has to have the same structure of the input, the objective is to learn a model able to reconstruct well (producing small reconstruction error) data coming from the same distribution of the training data.\n",
        "\n",
        "Different data (for example anomalies) should produce higher reconstruction error.\n",
        "\n",
        "In order to inncrease the data size in the Decoder part we can use the class\n",
        "**`UpSampling2D`**  https://keras.io/api/layers/reshaping_layers/up_sampling2d/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv6VPQTirAbu"
      },
      "outputs": [],
      "source": [
        "# Define the convolutional autoencoder model\n",
        "\n",
        "# input shape must be the same size as the images that will be fed into it by the generators\n",
        "# The output layer must be the same dimensions as the original image\n",
        "model = Sequential()\n",
        "#-------------------------\n",
        "\n",
        "# Encoder\n",
        "# TODO: define the encoder part as follows:\n",
        "#       - use (3,3) kernel size\n",
        "#       - use 3 conv layers (each layer followed by a MaxPooling layer)\n",
        "#       - use the following filters number path: 16, 8, 3\n",
        "#       - use padding='same'\n",
        "#       - use relu activation\n",
        "...\n",
        "\n",
        "# Decoder\n",
        "# TODO: replicate the Encoder structure but in the \"opposite\" way\n",
        "#       Use the sigmoid for last layer\n",
        "...\n",
        "#-------------------------\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teWhDR51ruJd"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model name\n",
        "model_filepath = 'image_anomaly_ae.keras'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-N87K0Nrv-s",
        "outputId": "fc0af51d-89dd-4c61-b26b-687cc546c2fa"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "\n",
        "# Early stopping (stops training when validation doesn't improve for {patience} epochs)\n",
        "# TODO: implement EarlyStopping\n",
        "# NOTE: see keras callbacks EarlyStopping\n",
        "es = ...\n",
        "\n",
        "# Saves the best version of the model to disk (as measured on the validation data set)\n",
        "# TODO: implement SaveBest callback\n",
        "# NOTE: see keras callbacks ModelCheckpoint\n",
        "save_best = ...\n",
        "\n",
        "# TODO: train model for 50 epochs using pre-defined callbacks\n",
        "history = model.fit(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training and validation losses during training phase\n",
        "plt.plot(history.history['loss'], label=\"Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"ValLoss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Phase\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZfYmKqTGJye"
      },
      "source": [
        "Training continues after improvement stops for the number of epochs equivalent to the 'patience' hyper-parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0-WehjjvKrb",
        "outputId": "ab3a0f6d-b65e-45b8-c4f7-6fbf75f5b3e0"
      },
      "outputs": [],
      "source": [
        "# To get back the model that performed best on the validation set we load the checkpointed model from disk.\n",
        "# TODO: load model from file\n",
        "model = ...\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "griOa5DBHLki"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot original image VS model reconstructed image\n",
        "def _plot_original_vs_predicted(dataset, title=\"\"):\n",
        "    # take 1 batch\n",
        "    for x, _ in dataset.take(1):\n",
        "        # TODO: use juste first 4 images as model input\n",
        "        predicted = ...\n",
        "        print(predicted.shape)\n",
        "        \n",
        "        # plot Original vs Predicted\n",
        "        fig, axs = plt.subplots(4, 2, figsize=(5,8))\n",
        "        for i in range(4):\n",
        "            # TODO: plot original image\n",
        "            axs[i][0].imshow(...)\n",
        "            # TODO: plot reconstructed image\n",
        "            axs[i][1].imshow()\n",
        "        \n",
        "        axs[0][0].set_title(\"Original\")\n",
        "        axs[0][1].set_title(\"Predicted\")\n",
        "        fig.suptitle(title)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: use '_plot_original_vs_predicted' function to check visually the reconstructed images\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs6up9dKHHKA"
      },
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQRPEVOsvs-X",
        "outputId": "4efdfcd8-53d3-4cd2-da6b-7e3671985059"
      },
      "outputs": [],
      "source": [
        "# We want the difference in error between the testing set (normal) images\n",
        "# and anomalous images to be as high as possible\n",
        "# TODO: evaluate the model on test set and anomaly set\n",
        "test_eval = ...\n",
        "anomaly_eval = ...\n",
        "\n",
        "print(f\"Error on test set:{test_eval:.3f}\")\n",
        "print(f\"Error on anomaly set:{anomaly_eval:.3f}\")\n",
        "print(f\"Difference: {abs(test_eval - anomaly_eval):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0IS2_g2FFFn"
      },
      "source": [
        "#### **Analysis of the reconstruction errors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute the reconstruction error for each picture\n",
        "def compute_plot_rec_error(dataset, plot=True, title=\"\", color=\"tab:green\"):\n",
        "    r_errors = []\n",
        "    # TODO: define the MSE as lambda function\n",
        "    _mse = ...\n",
        "\n",
        "    for batch_x, _ in dataset:\n",
        "        # TODO: get model prediction for actual batch\n",
        "        preds = ...\n",
        "        \n",
        "        # TODO: compute the MSE for every single image inside the batch\n",
        "        actual_errors = ...\n",
        "        r_errors.extend(actual_errors)\n",
        "\n",
        "    if plot:\n",
        "        plt.scatter(x=range(len(r_errors)), y=sorted(r_errors), s=3.0, c=color)\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return r_errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apple_rec_errors = compute_plot_rec_error(train_dataset_norm, title=\"Apple Rec. Error\", color=\"tab:green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eggplant_rec_errors = compute_plot_rec_error(anomaly_dataset_norm, title=\"Eggplant Rec. Error\", color=\"tab:red\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine them into a single plot\n",
        "all_rec_errs = tuple(zip(apple_rec_errors, [\"tab:green\"]*len(apple_rec_errors))) + tuple(zip(eggplant_rec_errors, [\"tab:red\"]*len(eggplant_rec_errors)))\n",
        "all_rec_errs = sorted(all_rec_errs, key=lambda x: x[0])\n",
        "\n",
        "plt.scatter(\n",
        "    x=range(len(all_rec_errs)),\n",
        "    y=[x for x, _ in all_rec_errs],\n",
        "    c=[y for _, y in all_rec_errs],\n",
        "    s=1.0,\n",
        "    )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "appletest_rec_errors = compute_plot_rec_error(test_dataset_norm, title=\"Apple(test) Rec. Error\", color=\"tab:green\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj2Bpd2AJc94"
      },
      "source": [
        "#### **Count anomalies on datasets given a threshold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_anomalies(rec_errors, threshold, name=\"\"):\n",
        "    count_anomaly = 0\n",
        "\n",
        "    # TODO: count items over the selected threshold\n",
        "    count_anomaly = ...\n",
        "\n",
        "    print(f\"{name} anomaly {count_anomaly} over a total of {len(rec_errors)} {name}\")\n",
        "    print(f\"{(count_anomaly/(len(rec_errors)) * 100):.2f} %\")\n",
        "\n",
        "    return count_anomaly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: try to define a threshold\n",
        "threshold = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH0Rym-IZ-qa",
        "outputId": "ea559e9e-645a-4480-c4f7-09b3f7424c43"
      },
      "outputs": [],
      "source": [
        "# Anomaly detection on apple_train samples\n",
        "apple_count_anomaly = count_anomalies(apple_rec_errors, threshold, name=\"apple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detection on apple_test samples\n",
        "appletest_count_anomaly = count_anomalies(appletest_rec_errors, threshold, name=\"apple (test)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt9U-_4bKofT",
        "outputId": "a875a5db-652b-4cfc-dee0-6ce52f2deba6"
      },
      "outputs": [],
      "source": [
        "# Count anomalies on eggplants\n",
        "#anomaly detection in the eggplant samples\n",
        "eggplant_count_anomaly = count_anomalies(eggplant_rec_errors, threshold, name=\"eggplant\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXUNdB1fJfzR"
      },
      "source": [
        "#### **ROC Curve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import auc,roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLJMup4wKmLe",
        "outputId": "4ad9ab15-1c2e-4320-e2cb-c4932b8ebb7a"
      },
      "outputs": [],
      "source": [
        "# Assign labels\n",
        "# Create labels for normal and anomaly samples\n",
        "apple_test_labels =  np.zeros(len(apple_rec_errors))     # normal label = 0\n",
        "eggplant_test_labels = np.ones(len(eggplant_rec_errors)) # anomaly label = 1\n",
        "\n",
        "# Put all the labels together\n",
        "all_labels = np.concatenate((apple_test_labels, eggplant_test_labels))\n",
        "\n",
        "# Put together the reconstruction errors and Target_scores\n",
        "all_errors  = apple_rec_errors + eggplant_rec_errors\n",
        "\n",
        "# Create a dataframe to store all the above information, to have everything together\n",
        "# This way we can compute some statistics easily\n",
        "error_df = pd.DataFrame({\n",
        "        'reconstruction_error': all_errors,\n",
        "        \"true_class\": all_labels,\n",
        "        },)\n",
        "\n",
        "display(error_df.describe())\n",
        "display(error_df.head())\n",
        "display(error_df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "L9QWAdnsMzNb",
        "outputId": "593c88ea-aa55-45a9-89d6-c92cb9002718"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n",
        "\n",
        "# AUC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.4f}')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0UTxf5RUg5L",
        "outputId": "430a6ec9-3009-4e26-cd7b-cfb588920d93"
      },
      "outputs": [],
      "source": [
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "\n",
        "print(\"Optimal Threshold:\", optimal_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = count_anomalies(apple_rec_errors, optimal_threshold, name=\"apple (opt)\")\n",
        "_ = count_anomalies(appletest_rec_errors, optimal_threshold, name=\"apple (opt)\")\n",
        "_ = count_anomalies(eggplant_rec_errors, optimal_threshold, name=\"eggplant (opt)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
        "\n",
        "for i, (rec_err, title) in enumerate(zip([apple_rec_errors, eggplant_rec_errors], [\"Apple\", \"Eggplant\"])):\n",
        "    axs[i].scatter(x=range(len(rec_err)), y=sorted(rec_err), s=3.0)\n",
        "    axs[i].axhline(optimal_threshold, color='tab:red', linestyle=\"--\", linewidth=.5)\n",
        "    axs[i].set_title(title)\n",
        "    axs[i].legend([f\"{title} rec. error\", \"Threshold\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
