{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1980ff",
   "metadata": {},
   "source": [
    "# Problem\n",
    "Recurrent neural networks can also be used as **generative models**.\n",
    "\n",
    "This means that in addition to being used for predictive models (making predictions) \n",
    "they can learn the sequences of a problem and \n",
    "then generate entirely new plausible sequences for the problem domain.\n",
    "\n",
    "In this lesson we are going to use the dataset: ``Aliceâ€™s Adventures in Wonderland``.\n",
    "\n",
    "We are going to learn the dependencies between characters and \n",
    "the conditional probabilities of characters in sequences \n",
    "so that we can in turn generate wholly new and original sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b952c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.saving import load_model\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b18b75",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96953ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from file\n",
    "dataset_path = \"wonderland.txt\"\n",
    "\n",
    "# TODO: read file, save content into raw_text var\n",
    "...\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace \"\\n\", \"\\t\", \"\\r\" with \" \" (space)\n",
    "# in this way we can easily split dataset just using spaces \n",
    "raw_text = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ced7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's count how many word our dataset is made up of\n",
    "# TODO: get all words (split by space)\n",
    "words = ...\n",
    "\n",
    "# TODO: get unique words\n",
    "unique_words = ...\n",
    "\n",
    "print(words)\n",
    "print(unique_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize our Encoder (words to int)\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=...,\n",
    "    # if needed we could work with ngrams too. Must specify number of ngrams\n",
    "    ngrams=None, # 3\n",
    ")\n",
    "\n",
    "# TODO: compute the vocabulary using adapt method\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get our vocabulary\n",
    "vocab = ...\n",
    "\n",
    "display(vocab[:100])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our sequence length\n",
    "sequence_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31984075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacking with sequences\n",
    "# TODO: get first sequence\n",
    "first_sequence = ...\n",
    "\n",
    "display(first_sequence)\n",
    "\n",
    "# TODO: convert first sequence (list[str]) into a string\n",
    "display(...)\n",
    "\n",
    "# TODO: get the encoding of that sequence\n",
    "display(encoder(\" \".join(first_sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our supervised dataset\n",
    "# we need an input text and a label\n",
    "# our input text will be a sentence (of length: sequence_len)\n",
    "# our label (or ground truth) will be next word\n",
    "# Eg: \n",
    "#    input text: Alice is taking a\n",
    "#    label:     nap\n",
    "dataset_x = []  # text input\n",
    "dataset_y = []  # labels\n",
    "\n",
    "for i in range(len(words)-sequence_len):\n",
    "    # TODO: get sequence\n",
    "    seq_input = ...\n",
    "    # TODO: get label\n",
    "    seq_output = ...\n",
    "\n",
    "    dataset_x.append(seq_input)\n",
    "    dataset_y.append(seq_output)\n",
    "    \n",
    "# TODO: numpy conversion\n",
    "dataset_x = ...\n",
    "dataset_y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_x.shape)\n",
    "print(dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81164eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_x.reshape(-1, 1) # reshape to right shape\n",
    "\n",
    "# labels are one-hot encoded\n",
    "# TODO: get y encoding\n",
    "y = ...\n",
    "# TODO: compute one-hot encoding\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb30730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print()\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: creating a Tensorflow Dataset from x and y\n",
    "dataset = ...\n",
    "\n",
    "display(dataset)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take and print first element of our dataset\n",
    "for text, label in dataset.take(1):\n",
    "    # text is an array made by one elem. It must be decoded to utf-8 to be \n",
    "    # displayed properly\n",
    "    print(\"IN:\", text[0].numpy().decode('utf-8'))\n",
    "    print(\"LABEL:\", label, f\"({np.argmax(label)} --> {vocab[np.argmax(label)]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "# using 5% of data as testing set\n",
    "testing_len = int(len(dataset)*0.05)\n",
    "training_len = len(dataset) - testing_len\n",
    "\n",
    "print(training_len)\n",
    "print(testing_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/test Tensorflow Datasets\n",
    "# dividing in batch, prefetching elements and shuffling training set\n",
    "# TODO: get training dataset, shuffle, divide in batch and apply prefetch\n",
    "train_dataset = ...\n",
    "# TODO: get testing dataset, divide in batch and apply prefetch\n",
    "test_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bfc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take and print first training batch\n",
    "for text, label in train_dataset.take(1):\n",
    "    print(text.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d6902",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405726d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our Keras model using LSTM layer\n",
    "model = tf.keras.Sequential([\n",
    "    # TODO: Encoder layer\n",
    "    ...,\n",
    "    # Trainable Embedding layer\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=..., # TODO\n",
    "        output_dim=256,\n",
    "        mask_zero=True,  # remember padding and masking\n",
    "    ),\n",
    "    # TODO:LSTM layer with 256 units\n",
    "    ...,\n",
    "    # Dropout layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # Output layer, using softmax\n",
    "    ...,\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f39761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "model_filepath=\"lstm_alice-mytraining.keras\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5, # Try with few epochs. We have a pre-trained model with 100 epochs\n",
    "    callbacks=[es_callback, checkpoint_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b130d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss during training phase\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "axis[0].plot(history.history['loss'])\n",
    "axis[0].legend([\"training loss\"])\n",
    "axis[1].plot(history.history['accuracy'], color=\"tab:orange\")\n",
    "axis[1].legend([\"training accuracy\"])\n",
    "fig.suptitle(\"Training phase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_output(model, train_dataset):\n",
    "    for text, label in train_dataset.take(1):\n",
    "        res = model.predict(text)\n",
    "        # TODO: convert one-hot encoding into token\n",
    "        pred_tokens = ...\n",
    "        \n",
    "        print(\"Input Text:\", text.shape)\n",
    "        print(\"Model Preds:\", res.shape)\n",
    "        print(\"Pred Tokens:\", pred_tokens.shape)\n",
    "        print()\n",
    "        \n",
    "        for i, t in enumerate(text):\n",
    "            print(t[0].numpy().decode('utf-8'))\n",
    "            print(\"PRED:\", vocab[pred_tokens[i]])\n",
    "            print(\"TRUE:\", vocab[np.argmax(label[i])])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some model preds\n",
    "test_model_output(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model_filepath=\"lstm_alice-pretrained-100e.keras\"\n",
    "# TODO: load pre-trained model\n",
    "model_pretrained = ...\n",
    "display(model_pretrained.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some model preds using pre-trained model\n",
    "test_model_output(model_pretrained, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23759cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_random_texts(model):\n",
    "    in_text = [\n",
    "        [\"The unicorn is flying into the\"],\n",
    "        [\"Monkey are very\"],\n",
    "        [\"Alice is taking a\"],\n",
    "        ]\n",
    "    res = model.predict(tf.convert_to_tensor(in_text))\n",
    "\n",
    "    for i, t in enumerate(in_text):\n",
    "        print(t[0], end=\" \")\n",
    "        print(vocab[np.argmax(res[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_random_texts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_random_texts(model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive(model, tokens=20, sentence=2):\n",
    "    sentences = [\n",
    "        \"Alice is taking a\",\n",
    "        \"Monkey are very\",\n",
    "        \"The unicorn is flying into the\",\n",
    "    ]\n",
    "    in_text = sentences[sentence]\n",
    "\n",
    "    for i in range(tokens):\n",
    "        res = model.predict(tf.convert_to_tensor([[in_text]]))\n",
    "        next_word = vocab[np.argmax(res)]\n",
    "        in_text += f\" {next_word}\"\n",
    "        \n",
    "    print(in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model in an autoregressive mode\n",
    "for i in range(3):\n",
    "    autoregressive(model, sentence=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing pre-trained model in an autoregressive mode\n",
    "for i in range(3):\n",
    "    autoregressive(model_pretrained, sentence=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa86e6",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_hub.layers import TokenAndPositionEmbedding, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92879fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model using Transformer layer\n",
    "model_transformer = Sequential([\n",
    "    # TODO: Encoder layer\n",
    "    ...,\n",
    "    # Embedding with PositionEncoding layer\n",
    "    # This layer creates a keras.layers.Embedding token embedding and a\n",
    "    # keras_hub.layers.PositionEmbedding position embedding and sums their output when called\n",
    "    TokenAndPositionEmbedding(\n",
    "        vocabulary_size=..., # TODO\n",
    "        sequence_length=..., # TODO\n",
    "        embedding_dim=256, # model dim (d)\n",
    "        mask_zero=True,\n",
    "    ),\n",
    "    # TODO: TransformerEncoder layer\n",
    "    #       feedforward network dim: 64\n",
    "    #       MultiHead attention heads: 8\n",
    "    TransformerEncoder(\n",
    "        ...,\n",
    "    ),\n",
    "    # Reduce tensor dimension by computing the mean over the temporal dimension (sequence length)\n",
    "    # Our TransformerEncoder layers give us a tensor of shape (batch_size, sequence_length, embedding_dim)\n",
    "    # That have an extra dimension: our final output shape must be (batch_size, vocab_length).\n",
    "    # vocab_length --> our label is one-hot encoded !!\n",
    "    # The final result will be (batch_size, embedding_dim)\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    # Dropout layer\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(len(vocab), activation='softmax'),\n",
    "])\n",
    "\n",
    "model_transformer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# want to check by yourself tensor shapes ?\n",
    "# uncomment the following code and try to comment some model's layers\n",
    "# model_transformer.predict(tf.convert_to_tensor([[\"im a test\"]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SaveBest callback\n",
    "model_filepath = \"transformers_alice-mytraining.keras\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_filepath,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_transformer.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,  # try using few epochs. We have a pre-trained version (100 epochs)\n",
    "    callbacks=[checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d51357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss during training phase\n",
    "fig, axis = plt.subplots(1, 2)\n",
    "axis[0].plot(history.history['loss'])\n",
    "axis[0].legend([\"training loss\"])\n",
    "axis[1].plot(history.history['accuracy'], color=\"tab:orange\")\n",
    "axis[1].legend([\"training accuracy\"])\n",
    "fig.suptitle(\"Training phase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model\n",
    "model_filepath=\"transformers_alice-pretrained-100e.keras\"\n",
    "# TODO: load pre-train model\n",
    "model_transformer_pretrained = ...\n",
    "display(model_transformer_pretrained.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some model preds\n",
    "test_model_output(model_transformer, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some model preds using pre-trained model\n",
    "test_model_output(model_transformer_pretrained, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d58d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_random_texts(model_transformer)\n",
    "test_model_random_texts(model_transformer_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model in an autoregressive mode\n",
    "for i in range(3):\n",
    "    autoregressive(model_transformer, sentence=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing pre-trained model in an autoregressive mode\n",
    "for i in range(3):\n",
    "    autoregressive(model_transformer_pretrained, sentence=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cc3b1b",
   "metadata": {},
   "source": [
    "## LSTM vs Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate models performance using testing dataset\n",
    "eval_lstm = ...\n",
    "eval_lstm_pretrained = ...\n",
    "eval_transformers = ...\n",
    "eval_transformers_pretrained = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b829978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM (mytrain):\", eval_lstm)\n",
    "print(\"LSTM (pre-trained):\", eval_lstm_pretrained)\n",
    "print(\"Transformers (mytrain):\", eval_transformers)\n",
    "print(\"Transformers (pre-trained):\", eval_transformers_pretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
