{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d3353c-6de3-4ebd-b61b-023e4b755c3d",
   "metadata": {
    "id": "16d3353c-6de3-4ebd-b61b-023e4b755c3d"
   },
   "source": [
    "# Keras Vs Pytorch\n",
    "\n",
    "In this lecture, we will explore two widely-used deep learning frameworks: [Keras](https://keras.io/) and [Pytorch](https://pytorch.org/). While both are powerful tools for building neural networks, they offer\n",
    "distinct features and approaches that make them suitable for different tasks and user preferences.\n",
    "\n",
    "Our task today involves implementing a Multi-Layer Perceptron (MLP) with one hidden layer to tackle the MNIST digit classification problem. MNIST is a well-known\n",
    "dataset of handwritten digits, making it an ideal choice for demonstrating classification tasks due to its simplicity and availability.<br><br>\n",
    "\n",
    "\n",
    "Following are some differences between Keras and Pytorch:\n",
    "\n",
    "|                 | **PyTorch** | **Keras**|\n",
    "|-----------------|-------------|----------|\n",
    "| **Target Audience**        | Researchers, experienced developers with need for flexibility              | Beginners, quick prototypers, industry practitioners|\n",
    "| **Flexibility**            | High; dynamic computation graph allows for custom operations and research  | Moderate; pre-built layers offer speed but limit some customization|\n",
    "| **Community & Ecosystem**  | Strong research community support, backed by Facebook (Meta)               | Extensive enterprise adoption, part of TensorFlow ecosystem, backed by Google|\n",
    "| **Deployment Capabilities**| Supports model exporting for integration into production systems           | Leverages TensorFlow's ecosystem for scalable and efficient deployment|\n",
    "| **Documentation & Resources** | Advanced materials and documentation for experienced developers         | Beginner-friendly resources and tutorials|\n",
    "| **Use Cases**                | Experimental projects, cutting-edge research, complex models             | Industry applications, rapid prototyping, enterprise use|\n",
    "| **Scalability**|Handles large-scale deployments with dynamic computation graph performance | Efficient scaling via TensorFlow's distributed training capabilities|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36eb7fd-f3bc-4f05-98f0-f32023b44512",
   "metadata": {
    "id": "a36eb7fd-f3bc-4f05-98f0-f32023b44512"
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde311a4-7f4e-4047-8b3d-ff4358f608e5",
   "metadata": {
    "id": "dde311a4-7f4e-4047-8b3d-ff4358f608e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecdf839-95b4-4f37-b164-14d44a0280b9",
   "metadata": {
    "id": "7ecdf839-95b4-4f37-b164-14d44a0280b9"
   },
   "outputs": [],
   "source": [
    "# TODO: load the MNIST dataset using the keras.datasets.mnist.load_data function\n",
    "(x_train, y_train), (x_test, y_test) = ...\n",
    "\n",
    "# TODO: define the class number and the input layer size\n",
    "num_classes = ...\n",
    "input_shape = ...\n",
    "\n",
    "# TODO: flatten the images\n",
    "x_train = x_train.reshape((...))\n",
    "x_test  = x_test.reshape((...))\n",
    "\n",
    "# TODO: normalize the images\n",
    "x_train = ...\n",
    "x_test  = ...\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# TODO: convert class vectors to binary class matrices (one-hot encoding)\n",
    "# HINT: use the keras.utils.to_categorical function\n",
    "y_train_onehot = ...\n",
    "y_test_onehot  = ...\n",
    "\n",
    "print(\"y_train shape:\", y_train_onehot.shape)\n",
    "print(\"y_train:\", y_train[0], y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca2f1b-506e-48fe-9478-5b2ec76d8a75",
   "metadata": {
    "id": "00ca2f1b-506e-48fe-9478-5b2ec76d8a75"
   },
   "outputs": [],
   "source": [
    "# TODO: define our model as follow:\n",
    "#       - Input Layer: 28*28 neurons\n",
    "#       - Hidden layer: 10 neurons, ReLU activation function\n",
    "#       - Output layers: 10 neurons, Softmax activation function\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=..., name=\"Input layer\"),\n",
    "        layers.Dense(..., name=\"hidden_layer\"),\n",
    "        layers.Dense(..., name=\"output_layer\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# or you can just 'add' some layers\n",
    "# model = Sequential()\n",
    "# mode.add(keras.Input(shape=input_shape))\n",
    "# model.add(layers.Dense(10, activation=\"relu\"))\n",
    "# model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1c1e2-d96b-4b42-8ec7-932bb20ee4bd",
   "metadata": {
    "id": "2ff1c1e2-d96b-4b42-8ec7-932bb20ee4bd"
   },
   "outputs": [],
   "source": [
    "# TODO: using the 'compile' function:\n",
    "#       - select the SGD algorimt as the Optimizer\n",
    "#       - use the Categorical Crossentropy Loss as the loss fucntion\n",
    "#       - add the Accuracy metric\n",
    "\n",
    "# NOTE: Cross entropy measures the difference between the predicted probability and the true probability.\n",
    "#       This makes the CrossEntropy Loss a good loss function for Classifiers !\n",
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb71b7-629a-470f-b15e-9087f90611e8",
   "metadata": {
    "id": "4abb71b7-629a-470f-b15e-9087f90611e8"
   },
   "outputs": [],
   "source": [
    "# TODO: using the .fit method, train the model as follows\n",
    "#       - batch size: 32\n",
    "#       - epochs 10\n",
    "history = model.fit(..., verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddc6c3-b820-478b-a351-48cdd2bdc9df",
   "metadata": {
    "id": "1fddc6c3-b820-478b-a351-48cdd2bdc9df"
   },
   "outputs": [],
   "source": [
    "# TODO: plot the Accuracy and the Training Loss stored in the HISTORY var\n",
    "plt.plot(...)\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(..., color='tab:orange')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57a05a-9e4c-4e98-8caf-825589ee7a9d",
   "metadata": {
    "id": "9e57a05a-9e4c-4e98-8caf-825589ee7a9d"
   },
   "outputs": [],
   "source": [
    "# TODO: evaluate the model using the testing set\n",
    "# HINT: check the evaluate method !\n",
    "score = ...\n",
    "\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7RmnodtraURu",
   "metadata": {
    "id": "7RmnodtraURu"
   },
   "outputs": [],
   "source": [
    "# TODO: make some predictions\n",
    "# HINT: use the predict method\n",
    "preds = ...\n",
    "preds_labels = ...\n",
    "\n",
    "print(\"Prediction shapes:\", preds.shape, preds_labels.shape)\n",
    "print(\"Predicted label:\", preds_labels[0], preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a79c30-e9b6-4a3b-ae85-b259a6c64b84",
   "metadata": {
    "id": "93a79c30-e9b6-4a3b-ae85-b259a6c64b84"
   },
   "outputs": [],
   "source": [
    "# This is a cool way to preprocess data using Keras !\n",
    "(x_train_test, _), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "flat_and_normalize = keras.Sequential([\n",
    "    keras.Input(shape=(28,28)),\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Reshape((28*28,)),\n",
    "])\n",
    "\n",
    "_norm_dst = flat_and_normalize(x_train_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(_norm_dst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c9088-ddcd-4ea1-83c5-e9be5adb597b",
   "metadata": {
    "id": "aa5c9088-ddcd-4ea1-83c5-e9be5adb597b"
   },
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec32a1-c8d8-4423-b036-a14c2101b6c2",
   "metadata": {
    "id": "cdec32a1-c8d8-4423-b036-a14c2101b6c2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hz-VHFSV_S4F",
   "metadata": {
    "id": "hz-VHFSV_S4F"
   },
   "outputs": [],
   "source": [
    "# We are going to use the same dataset as before\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "print(\"y_train shape:\", y_train_onehot.shape)\n",
    "print(\"y_train:\", y_train[0], y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ppmhYboB632",
   "metadata": {
    "id": "4ppmhYboB632"
   },
   "outputs": [],
   "source": [
    "# TODO: define the Dataset class to handle images and labels\n",
    "class MyMnistDataset(Dataset):\n",
    "  def __init__(self, images, labels):\n",
    "    assert len(images) == len(labels)\n",
    "\n",
    "    # TODO: define the class attributes:\n",
    "    #       - images and labels\n",
    "    ...\n",
    "\n",
    "  def __len__(self):\n",
    "    # TODO: return the dataset len\n",
    "    # HINT: you can use images or labels to compute the dataset len\n",
    "    return ...\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # TODO: get image and label by given index\n",
    "    _selected_img = ...\n",
    "    _selected_lab = ...\n",
    "\n",
    "    # TODO: convert to tensor\n",
    "    _selected_img = ...\n",
    "    _selected_lab = ...\n",
    "\n",
    "    return _selected_img, _selected_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p_7eGQbgAota",
   "metadata": {
    "id": "p_7eGQbgAota"
   },
   "outputs": [],
   "source": [
    "# TODO: create the training and testing datasets\n",
    "train_dataset = ...\n",
    "test_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3iodEXJCBVQx",
   "metadata": {
    "id": "3iodEXJCBVQx"
   },
   "outputs": [],
   "source": [
    "# TODO: create the training Dataloader using as batch size 32 with shuffle\n",
    "#       Create the testing Dataloader with batch size 1 (usefull for evaluation phase)\n",
    "#       no shuffle\n",
    "train_loader = DataLoader(...)\n",
    "test_loader = DataLoader(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec757959-2213-4d1c-9ddd-f9f0c1d68755",
   "metadata": {
    "id": "ec757959-2213-4d1c-9ddd-f9f0c1d68755"
   },
   "outputs": [],
   "source": [
    "# Choose device to use\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS device for computation.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device for computation.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90248f63-1537-4872-a02f-6f7a51ef0d85",
   "metadata": {
    "id": "90248f63-1537-4872-a02f-6f7a51ef0d85"
   },
   "outputs": [],
   "source": [
    "# TODO: define MLP model as follows:\n",
    "#       - Hidden layer: 28*28 Input neurons, 10 output neurons, ReLU activation function\n",
    "#       - Output layers: 10 Input neurons, 10 output neurons, Softmax activation function\n",
    "# HINT: check the torch.nn module !\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # initialize the base class (nn.Module) to inherit its methods and properties\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        # TODO: define activation functions\n",
    "        self.relu = ...\n",
    "        self.softmax = ...\n",
    "\n",
    "        # TODO: first fully connected layer: 28*28 or 784 input features, 10 output features\n",
    "        self.hidden_layer = ...\n",
    "\n",
    "        # TODO: output layer: 10 input features, 10 output features (number of classes)\n",
    "        self.output_layer = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: pass the input x through first fully connected layer\n",
    "        # HINT: the class attributes are callable (they are actualluy functions)!\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        # TODO: pass the input x through output layer\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        # return the output logits of shape (batch_size, 10), representing class scores\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d7df8-b2b3-4be6-a627-d1a54f666950",
   "metadata": {
    "id": "2c2d7df8-b2b3-4be6-a627-d1a54f666950"
   },
   "outputs": [],
   "source": [
    "# TODO: create the model and send it to the selected device\n",
    "model = ...\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5853b6-b8f4-44d1-94b3-bba4ace97aa6",
   "metadata": {
    "id": "4a5853b6-b8f4-44d1-94b3-bba4ace97aa6"
   },
   "outputs": [],
   "source": [
    "# TODO: define the Loss Function as the CrossEntropy Loss\n",
    "criterion = ...\n",
    "# TODO: define the optimizer as the SGD algorithm with learning rate 0.01\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9Oi1wcF7C-",
   "metadata": {
    "id": "3c9Oi1wcF7C-"
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "model.train() # prepare the model for training\n",
    "\n",
    "history = []\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    epoch_loss = []\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for x, y in train_loader:\n",
    "      # TODO: send data to selected device\n",
    "      x, y = ...\n",
    "      # clear the gradients of all optimized variables\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # TODO: forward pass, compute predicted outputs by passing inputs to the model\n",
    "      output = ...\n",
    "\n",
    "      # TODO: calculate the loss\n",
    "      loss = ...\n",
    "\n",
    "      # TODO: backward pass, compute gradient of the loss with respect to model parameters\n",
    "      loss.backward()\n",
    "\n",
    "      # perform a single optimization step (parameter update)\n",
    "      optimizer.step()\n",
    "\n",
    "      # update running training loss\n",
    "      epoch_loss.append(loss.item())\n",
    "\n",
    "    # print training statistics\n",
    "    # calculate average loss over an epoch\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    history.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {epoch_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NMvPLDoXGuUC",
   "metadata": {
    "id": "NMvPLDoXGuUC"
   },
   "outputs": [],
   "source": [
    "# Plot Loss Function over epochs\n",
    "plt.plot(history)\n",
    "plt.title(\"Loss Function\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UewWTJ93IFhp",
   "metadata": {
    "id": "UewWTJ93IFhp"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval() # prepare model for evaluation\n",
    "accuracy = 0\n",
    "\n",
    "# prevent computing gradients\n",
    "with torch.no_grad():\n",
    "  for x, target in test_loader:\n",
    "    output = model(x)\n",
    "\n",
    "    # compute the maximum on the second dimension (remember we have a batch size = 1)\n",
    "    # and return a tuple: (values, indices)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    _, target = torch.max(target, 1)\n",
    "\n",
    "    accuracy += (pred == target).item()\n",
    "\n",
    "accuracy = accuracy / len(y_test)\n",
    "print(f\"Accuracy: {accuracy*100}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "153Itfr8cSPhVgOLSyxUseSs0YqV7JuQQ",
     "timestamp": 1741128506907
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
