{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# **Text classification with an RNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from IPython.display import display, Markdown\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "pd.set_option('display.max_columns', 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "# Helper function to plot charts\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "### Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a `positive (1)` or `negative (0)` sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHRwRoP2nVHX"
      },
      "outputs": [],
      "source": [
        "# download and load the dataset\n",
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "\n",
        "# TODO: extract available sets (training, testing, unsupervised)\n",
        "train_dataset, test_dataset, unsupervised = ...\n",
        "\n",
        "# print data structure\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print some dataset's infos\n",
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd4_BGKyurao"
      },
      "outputs": [],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy().decode(\"utf-8\"))\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Validation Split\n",
        "# TODO: compute the training size length. Must be the 80% of training set\n",
        "train_size = ...\n",
        "\n",
        "# TODO: get the validation set using the skip method\n",
        "# skip [train_size] elements and take remaining\n",
        "val_dataset = ...\n",
        "\n",
        "# TODO: get the training set using the take method\n",
        "# take first [train_size] elements\n",
        "train_dataset = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "# BUFFER_SIZE = 10000\n",
        "TRAIN_BUFFER_SIZE = len(train_dataset)\n",
        "VAL_BUFFER_SIZE = len(val_dataset)\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "# we are using 'prefetch' to accelerate the training phase by pre-loading next needed batch while current is\n",
        "# being still in use. In this way we can avoid dead times in training phase (more or less)\n",
        "\n",
        "# The shuffle methods needs a BUFFER SIZE in input because it loads a buffer of N elements and then shuffles this elements.\n",
        "# In order to have an exact shuffle we need to set BUFFER_SIZE = len(actual_dataset)\n",
        "# TODO: \n",
        "#   - shuffle training/validation sets\n",
        "#   - Subdivide in batch\n",
        "#   - use the prefetch method\n",
        "train_dataset = ...\n",
        "val_dataset = ...\n",
        "test_dataset = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqkvdcFv41wC"
      },
      "outputs": [],
      "source": [
        "# Take fist batch and display first 3 elements (sentence, label)\n",
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "### **Create the text encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "# We are setting a Max Vocabulary size of 1000 words. This means our vocabulary is capped at 1000 words\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# TODO: set max number of token to VOCAB_SIZE\n",
        "# The TextVectorization layer maps a text into a integer sequence\n",
        "encoder = tf.keras.layers.TextVectorization(...)\n",
        "\n",
        "# The adapt function will analyze the dataset, determine the frequency of individual\n",
        "# string values, and create a vocabulary from them.\n",
        "# The processing of each example contains the following steps:\n",
        "#   1. Standardize applying: lowercasing + punctuation stripping\n",
        "#   2. Split each example into words\n",
        "#   3. Recombine words into tokens (usually ngrams)\n",
        "#   4. Index tokens (associate a unique int value with each token)\n",
        "#   5. Transform each example using this index into a vector of ints \n",
        "\n",
        "# TODO: use the graining set to construct the vocabulary\n",
        "encoder.adapt(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are some vocabulary tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBoyjjWg0Ac9",
        "outputId": "30559918-a31e-4992-bd9e-d084fd11f980"
      },
      "outputs": [],
      "source": [
        "# TODO: get the vocabulary from encoder\n",
        "vocab = ...\n",
        "display(vocab)\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGc7C9WiwRWs"
      },
      "outputs": [],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "display(encoded_example[0])\n",
        "print(encoded_example.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are two main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_tD0QY5wXaK"
      },
      "outputs": [],
      "source": [
        "for n in range(3):\n",
        "  original = example[n].numpy().decode(\"utf-8\")\n",
        "  reenc = \" \".join(vocab[encoded_example[n]])\n",
        "\n",
        "  soriginal = original.split(\" \")\n",
        "  sreenc = reenc.split(\" \")\n",
        "\n",
        "  print(f\"Example {n}\")\n",
        "  df = pd.DataFrame({\"original\": soriginal, \"reenc\": sreenc[:len(soriginal)]})\n",
        "  display(df.T)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## **Create the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "# TODO: define our model using LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    # TODO: encoding layer\n",
        "    ...,\n",
        "    # TODO: Embedding layer\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=...,\n",
        "        output_dim=64,\n",
        "        # TODO: enable masking\n",
        "\n",
        "        ),\n",
        "    # TODO: LSTM 64\n",
        "    ...,\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer uses masking to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87a8-CwfKebw"
      },
      "outputs": [],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To find out more check the Tensorflow notebook: [Padding and Masking](https://www.tensorflow.org/guide/keras/masking_and_padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy'],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "### **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(test_dataset)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for original in test_dataset.take(1):\n",
        "    # TODO: get original label\n",
        "    y_true = ...\n",
        "\n",
        "    # TODO: get model prediction\n",
        "    preds = ...\n",
        "    # TODO: check which preds are positive (1) or negative (0)\n",
        "    y_pred = ...\n",
        "\n",
        "    print(\"y_true:\", y_true.numpy().tolist())\n",
        "    print(\"y_pred:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Glove 6B 50 dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download pre-trained Glove embedding file from Github and extract it\n",
        "# WARNING: it will download something like 1GB, so it could take a while !!\n",
        "#!wget https://github.com/uclnlp/inferbeddings/raw/refs/heads/master/data/glove/glove.6B.50d.txt.gz\n",
        "#!gzip -dkf glove.6B.50d.txt.gz\n",
        "#!wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read embedding file\n",
        "# embedding file is made by rows following the format:\n",
        "#   world embedding(world)\n",
        "embeddings_index = {}\n",
        "glove_dims = 50\n",
        "with open(f\"glove.6B.{glove_dims}d.txt\",\"r\") as f:\n",
        "    for line in f:\n",
        "        values = line.split(\" \")\n",
        "        word = values[0]\n",
        "        embedding = np.array(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "display(embeddings_index)\n",
        "print(len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: get the \"house\" world's embedding\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: compute the embedding matrix\n",
        "# The embedding matrix is built as follows:\n",
        "#   - number of rows = our vocabulary length\n",
        "#   - each row is the Glove embedding\n",
        "# Row 0 means: \"give me de Glove embedding of the word indexed by 0 in my vocabulary\"\n",
        "embeddings_matrix = np.zeros((..., ...))\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# in this way we are getting word - word's index in our vocabulary\n",
        "for index, world in enumerate(encoder.get_vocabulary()):\n",
        "    embedding_vector = embeddings_index.get(world)\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[index] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "\n",
        "print(f\"Converted {hits} words ({misses} misses)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: use Glove embedding\n",
        "model_glove = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=...,\n",
        "        output_dim=...,\n",
        "        # TODO: disable training for this layer\n",
        "\n",
        "        # TODO: add embedding matrix\n",
        "\n",
        "        mask_zero=True,\n",
        "        ),\n",
        "    tf.keras.layers.LSTM(glove_dims),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_glove.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy'],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model_glove.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = model_glove.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = model_glove.predict(test_dataset.take(1))\n",
        "y_preds = [1 if x >= 0 else 0 for x in preds]\n",
        "\n",
        "for x, y_ture in test_dataset.take(1):\n",
        "    for i in range(3):\n",
        "        print(x[i].numpy().decode(\"utf-8\"))\n",
        "        print(\"y_pred\", y_pred[i])\n",
        "        print(\"y_true\", y_true[i].numpy())\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Unsupervised Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "unsupervised_dataset = unsupervised.batch(1).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sentence in unsupervised_dataset:\n",
        "    display(Markdown(\"## Sentence\\n\"+sentence[0][0].numpy().decode(\"utf-8\")))\n",
        "    user_input = input()\n",
        "\n",
        "    y_pred = model.predict(sentence)\n",
        "    y_pred_glove = model_glove.predict(sentence)\n",
        "\n",
        "    print(\"Your label is: \", user_input)\n",
        "    print(\"Model label is: \", int(y_pred[0][0] >= 0))\n",
        "    print(\"Model (glove) label is: \", int(y_pred_glove[0][0] >= 0))\n",
        "    time.sleep(0.1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
